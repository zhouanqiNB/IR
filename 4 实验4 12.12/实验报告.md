# 信息检索hw4实验报告

> 1911590 周安琪

# 1 实验成品整体介绍

![image-20211208033356068](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208033356068.png)

我的成品分为4个模块，将在实验报告的不同部分进行详细介绍

# 2 文件夹结构

- 实验报告.pdf

- 讲解视频.mp4

- 1 爬取数据和文本索引

  - nku12club.py---爬取了站点所有的目录页，获取了所有详情页的url，建立索引club12。
  - detailUrls.txt---由nku12club.py生成。
  - nku12clubDetail.py---爬取了所有详情页的内容，建立索引club12_detail。
  - nku12clubMenu.py---爬取了所有目录页的内容，建立索引club12_menu。
  - detailRescue.py---爬取过程中detailUrls.txt消失、网络出现问题，此文件用于恢复文件并且完成剩下来的爬取。

- 2 pageRank

  - pageRank.py---手动构建矩阵，算出value数组，输出到pageRankValue.txt
  - pageRankValue.txt---由pageRank.py生成
  - pageRank2.py---读入pageRankValue.txt，将value和url做match，生成pageRankDic.txt
  - pageRankDic.txt---由pageRank2.py生成

- 3 查询服务

  - 前端页面

    - index.php---全局搜索的前端

      ![image-20211208034848981](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208034848981.png)

      ![image-20211208034904812](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208034904812.png)

    - high.php---精准搜索的前端

      ![image-20211208034934941](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208034934941.png)

      ![image-20211208034954768](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208034954768.png)

    - high2.php---高级搜索的前端

      ![image-20211208035036398](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208035036398.png)

      ![image-20211208035050624](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208035050624.png)

    - main.php

      位于yii框架views/layouts/下，主要是写了按钮以及头尾。

  - python脚本

    - test.py---被index.php调用，用于计算index.php的结果并且写入日志
    - test2.py---被high.php调用，用于计算high.php的结果并且写入日志
    - test3.py---被high2.php调用，用于计算high2.php的结果并且写入日志

- 4 个性化查询--查询历史

  - history.php---查询历史的前端

    ![image-20211208035216171](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208035216171.png)

  - readHistory.py---被history.php调用，从history.txt文件里读出内容显示到网页上

  - history.txt---查询脚本在运行的时候将日志写到本文件。

# 3 网页抓取和文本索引

最终建立了三个索引：

- club12---用于高级查询1
- club12_detail---用于高级查询2和普通查询
- club12_menu---没用上

我主要是抓取了南开大学12社区的网页，共包含目录页107个，详情页2080个。我抓完数据并且做好处理之后才发现站长不希望被抓取，但是由于时间有限，来不及换一个站点抓取重新做处理，所以还是基于动漫站做的作业。

网页抓取和文本索引是一起做的，我的主要思路如下：

## 3.1 nku12club.py

> 建立了索引club12。

这个文件抓取了所有的目录页的html源码，由于本站的目录页url格式统一所以可以很简单地遍历。

同时在一个目录页上存在着若干个详情页的链接，我将这些url提取出来，并且存储在`detailUrls.txt`中，以待后续使用。

基于107个目录页的2080个条目我建立了一个不完全、但字段较为丰富的全站资源索引（包含动漫名称、连接、最新更新、更新时间、字幕组、标签）。

==抓取代码==（会得到整个html页面的源码）：

```python
url="http://12club.nankai.edu.cn/programs?category_id="+str(category)+"&order=update&page="+str(index)
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36'
}
response=requests.get(url=url,headers=headers)
page_text=response.text # 得到html的源码
```

==提取页内url==主要是用了正则表达式，每遍历一个目录页都做一次url提取然后放到大列表里，最后把列表写回`detailUrls.txt`：

```python
import re
def find_all(html="hello"):
    '''
    抽取html中的链接
    '''
    compile_rule=re.compile(r"<a.*?href=https://|http://.*? ")
    url_list=re.findall(compile_rule, html)
    return url_list;
```

![image-20211207234022195](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211207234022195.png)

==提取html中的文本==部分

这里我调了一个包

```python
from selectolax.parser import HTMLParser
...
text = HTMLParser(page_text).text()	# 提取文字
```

本文件中最主要的代码是==数据处理==，但由于过于繁琐，只展示索引结构。这是相对来说处理得比较细的一个索引，也是着手做作业最开始的成果，虽然后来发现好像不该这么做，但还是保留了，并且应用在最终的`12社区高级搜索1`模块中。

一个目录页大概是这样：

![image-20211207234222306](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211207234222306.png)

最终ES中的索引结构如下：

![image-20211207233616317](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211207233616317.png)

最后以存入ES索引中，随后==休眠==15秒继续爬下一个网页。

## 3.2 nku12clubDetail.py

> 建立了索引club12_detail。

这个文件先是从`detailUrls.txt`中读出了所有要遍历的详情页，然后花费十几个小时对两千多个网页做了抓取，建立的索引也是简单粗暴分为`title`, `link`, `content`三个字段：

![image-20211207234854887](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211207234854887.png)

## 3.3 nku12clubMenu.py

> 建立了索引club12_menu

这个文件是遍历了一遍目录页然后和nku12clubDetail.py一样建立了`title`, `link`, `content`三个字段的索引：

![image-20211207235401471](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211207235401471.png)

这里犯了一个错误：menu和detail的数据应该存在一个索引下面的，不仅方便而且格式也完全一致。由于分了两个索引，这直接导致在提供查询服务的时候仅搜索了detail页面，但对于结果我觉得影响不大。

## 3.4 难点

- 从来没用过爬虫，上手花了一些时间
- 爬详情页的时候，程序从当天下午爬到第二天，结果早上爬起来一看爬到1900多页的时候出了网络问题。更离谱的是detailUrls.txt原地失踪，这不是第一次发生在我电脑上了。只好很对不起站长地再爬了一次目录页，然后恢复到出问题的那个url继续爬取（多亏有控制台输出）。

# 4 链接分析

## 4.1 pageRank.py

链接分析是通过分析各个网页之间的指向关系来确定哪个网页比较重要，由于本站的结构比较简单而且也不好一而再再而三去爬数据，所以我直接==手动构建了矩阵==。

因为有107个目录页和2080个详情页，所以我做了一个大小为2187的矩阵。

因为我使用的`fast_pagerank`包只要求提供所有指向关系的二元组，就可以计算每个节点的pagerank数值。过程不太难，只是构建比较繁琐。

链接之间的指向关系主要是基于以下的五点观察：

- 网站结构基本上是：类别->目录->详情
- 每个目录页都可以指向同一种类别的任何目录页
- 每个目录页都可以指向另一种类别的第一个目录页
- 每个目录页指向相应的20个详情页（最后一个目录页可能不满，所以我统计了最后一个目录页的条目数来解决这个问题）
- 每个详情页都可以指向任意类别的第一个目录页

建立好矩阵后调包：

```python
from fast_pagerank import pagerank
from fast_pagerank import pagerank_power
...
A = np.array(pageRankVector)
weights=[]		
for i in range(0,len(pageRankVector)): # 给初始权重
	weights.append(1)

print(len(pageRankVector))
G = sparse.csr_matrix((weights, (A[:,0], A[:,1])), shape=(2187, 2187))
pr=pagerank(G, p=0.85)	#pr数组算出来
```

最后把pageRank的数值写进`pageRankValue.txt`，在pagerank2.py中我把它和所有的链接做了关联：

```python
with open("./pageRankValue.txt",'w',encoding='utf-8') as fp:
	for i in pr:
		fp.writelines(str(i))
		fp.writelines("\n")
```

输出文件：

![image-20211208001206459](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208001206459.png)

> 链接指向关系的构建应该是在爬数据的时候就做好，而且爬数据应该是递归的，我在本次作业中采用的思路过于简单粗暴了。

## 4.2 pageRank2.txt

把链接和pageRank连接在了一起，输出到`pageRankDic.txt`，以供后续使用。

输出文件：

![image-20211208001337756](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208001337756.png)

顺带一提，PageRank的结果是符合人的一般认知的。

比如在图中，“动画”类别的第一个目录页的数值会比普通的目录页大，这是因为事实上所有页面都可以跳转到这一页。同理其他类别的第一页也是如此，所有类别的第一页的pageRank值是一致的。

而且因此，所有第一页目录条目上的资源详情页的数值也比普通的详情页大。

但问题在于，==这种打分对本站意义不大==。比如我并不认为目录第一页上展示的动漫就一定跟别的动漫作品有什么区别，因为本站是按照更新时间排序的。所以在后续中，我给pageRankValue的权重很小。

## 4.3 难点

- pageRank的概念
- fast_pagerank包的使用

# 5 查询服务

查询服务分为四个版块，“我的历史查询”版块在下一节中介绍。

![image-20211208003106069](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208003106069.png)

## 5.1 12社区资源搜索

> 基于club12_detail索引

首先用户在web页面输入查询关键字，点击搜索按钮之后这个关键字将会被php变量接收并且传参给python脚本运行，随后页面上打印出来的结果都是由python程序打印的。前端页面将在下下节介绍，这里主要讲一下python程序`./3 查询服务/Python脚本/test.py`。

### 5.1.1 ES查询

python接收到参数后，将会简单地用下面的请求体在12club_detail中查询，获得查询结果：

```python
body = {
    "query": {
        "match": { "content": params } 
    },
}
res=es.search(index="club12_detail", body=body)
```

查询结果包含ES评分最高的十条记录，到这里还没有加入pagerank数值。

### 5.1.2 加入pageRank数值

由于我认为pagerank数值的意义不大，所以我仅仅是将这个数值加入了查询结果中（fast_pagerank包对pagerank的结果做了正规化，所以每个值都很小），并且算出了finalScore = score + pageRank，最后根据finalScore重新排序。

PS: pageRank的数值我直接是从`pageRankDic.txt`中读入新建的字典`dic`中。

```python
for i in range(0,len(a)):
    a[i]["pageRank"]=dic[a[i]["link"]]	# dic: "http://12...":"0.00232
for i in range(0,len(a)):
    a[i]["finalScore"]=a[i]["pageRank"]+a[i]["score"]
```

下图是我在debug的时候截的图（出现图中结果时我是按照finalScore正序排列的，很快我就改成了倒序）。排序代码如下：

```python
a = sorted(a, key=lambda k: k['finalScore'],reverse=True) 
```

![image-20211207204107080](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211207204107080.png)

### 5.1.3 表格打印

最后利用python输出html代码实现表格的打印，这里不重要，仅截图：

![image-20211208004429010](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208004429010.png)

## 5.2 12社区高级搜索1

> 基于club12索引

和高级搜索的概念不符，但没有删掉或者改名。

在nku12club.py中，利用目录页信息构建了如下图的索引，约2000条：

<img src="C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211207233616317.png" alt="image-20211207233616317" style="zoom: 50%;" />

我挑出了五个比较有价值的字段做了如图的查询服务，逻辑关系是简单的“与”：

![image-20211208005344240](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208005344240.png)

python程序`./3 查询服务/Python脚本/test2.py`一共会接收到5个参数，它会判断哪些是有效的，并且记录有效参数的数目

- 0：直接match_all
- 1：直接match
- \>=2：用bool查询

代码如下：

```python
body = {# if语句主要是对请求体做更改
    "query": {
        "match_all": {} 
    },
}

paramList=[title,updateNum,date,fansub,tag]

strList=["标题","最新更新","更新日期","字幕组","标签"]

if paramNum==0:
    pass
elif paramNum==1:
    del(body["query"]["match_all"])
    for i in range(0,5):
        if paramList[i]!="empty":
            body["query"]["match"]={strList[i]:paramList[i]}
            break
else:
    del(body["query"]["match_all"])
    body["query"]["bool"]={}
    body["query"]["bool"]["must"]=[]
    for i in range(0,5):
        if paramList[i]!="empty":
            body["query"]["bool"]["must"].append(
                {"match":{strList[i]:paramList[i]}})
```

这是制定了字幕组和更新日期之后的结果：

![image-20211208020943053](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208020943053.png)

## 5.3 12社区高级搜索2

> 基于club12_detail索引

实现了包含某几个关键词、并且同时不包含另几个关键词的功能，只是使用了ES简单的布尔查询+PageRank（PageRank部分和普通查询一样）。与、或的实现逻辑与之类似，但由于时间原因没有实现。

脚本位于`./3 查询服务/Python脚本/test3.py`

```python
body = {
    "query": {
        "bool": {
            "must":[],
            "must_not":[]
        } 
    },
}

for i in must:
    body["query"]["bool"]["must"].append({"match":{"content":i}})
for i in notnot:
    body["query"]["bool"]["must_not"].append({"match":{"content":i}})

res=es.search(index="club12_detail", body=body)
```

![image-20211208035036398](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208035036398.png)

![image-20211208035050624](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208035050624.png)

# 6 个性化查询--历史查询

## 6.1 实现

在用户的任意一个查询之后，用于查询的python脚本将会把输出到前端的内容以追加形式输出到history.txt中（所以是html格式的，而且最新的记录会显示在最下面）。在点击历史页面的时候，php调用readHistory.py脚本，在页面上显示之前的查询结果。

查询脚本的输出：

![image-20211208030204150](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208030204150.png)

history.txt：

![image-20211208041245687](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208041245687.png)

readHistory.py：

![image-20211208030243457](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208030243457.png)

## 6.2 效果

![image-20211208030941589](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208030941589.png)

# 7 Web界面

## 7.1 实现

这部分我主要是通过输入框接收用户输入的关键字，然后把其作为参数传给python脚本，由python来得出结果并且打印。并没有使用yii框架的MVC，之所以在框架下跑只是因为想用模板。

传参代码：

```php
<?php
    if( $_POST["submit"] == "Search"){//如果点击了search按钮
    $params = $_POST['keyword']; // 给参数赋值
    //用了绝对路径因为yii比较难搞
    $path="python3 /mnt/c/Users/16834/Desktop/NKUSearch/test.py "; //末尾要加一个空格
	//等同于命令`python python.py 参数`，并接收打印出来的信息，把python print出来的信息打印在前端页面上
    passthru($path.$params);
}
```

python接收参数：

```python
params = sys.argv[1] #即为获取到的PHP传入python的入口参数
print("这是关键字<font color='LightYellow'><b>"+params+"</b></font>的搜索结果：")
```

![image-20211208001827455](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211208001827455.png)

## 7.2 难点

- 从输入框里获取文字。

  因为我在互联网课程作业中主要负责的是调用数据画图所以根本不懂这一部分，最后我虽然仍然是在yii文件夹里写的，但是并没有使用yii框架的model，单纯地用php获取了关键字。

- php和python连接。

- xampp和elasticsearch不在一个系统下。

  在我起初调试的时候，总是无法输出结果，但是命令行直接运行python脚本证明python代码是没有问题的。查看apache的错误日志之后发现是因为我的ES是在wsl下装的，和win10连不起来。
  
  我把xampp迁移到wsl才解决了这个问题。
  
- php和python交互中输出乱码，这是因为二者编码不一致。

- php给python传多个参数很容易出错。

# 8 总结

共完成了指导书中要求的以下模块：

- 资源抓取

- 索引构建

- 链接分析（手动构建矩阵）

- 查询服务

  - 站内查询
  - 文档查询
  - 短语查询
  - 通配查询（一部分）
  - 查询日志
  - 逐字段查询

- 个性化查询（似乎没有将查询历史应用到查询过程中，但用户需要的话可以直接去看。）

- Web界面

  没有把完整的项目打包，所以应该跑不起来。