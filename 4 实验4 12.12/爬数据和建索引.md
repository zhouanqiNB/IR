# NK12社区

## 目录页

> 很好我已经美美爬完了数据并且打算存到ES里面了

查看刚建立的索引：

```bash
bin/elasticsearch
curl 'localhost:9200/_cat/indices?v'
```

![image-20211206020724313](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211206020724313.png)

> 是的我真的很蠢我第一次竟然弄了5个ES索引、、、、、、好的第二次我放到一起了。

查询：

```bash
curl -XPOST 'localhost:9200/club12/_search?pretty' -H 'Content-Type: application/json' -d '{"query": { "match": {"标题":"Epub格式"} }}'
```

这里我就只爬了6个模块，后面的音乐室省略了

- 动漫
- 漫画
- 音乐
- 游戏
- 小说
- 视频

```bash
# 删除索引
curl -XDELETE 'localhost:9200/club12'
```

## 详情页

然后我从上面收集到链接，也就是详情页，再建了一个detail索引。

```bash
curl -XPOST 'localhost:9200/club12_menu/_search?pretty' -H 'Content-Type: application/json' -d '{"query": { "match": {"content":"排球少年"} }}'

curl -XPOST 'localhost:9200/club12/_search?pretty' -H 'Content-Type: application/json' -d '{"query": { "match_all": {} }}'
```

> 。。。因为设置了sleep(15)，一个小时只能爬240个详情页。

## 我一共做了三个索引

一个是处理得很好的

剩下的是粗处理的详情和菜单



## 大问题

我一共有2000个网页要爬，我晚上睡觉之后我电脑继续爬，结果就connection error了，妈的

```bash
http://12club.nankai.edu.cn/programs/1228#intro
{
    "title":"[轻小说]-断罪的EXCEED",
    "link":"http://12club.nankai.edu.cn/programs/1228#intro",
    "content":"[轻小说]-断罪的EXCEED,其他名称:,字幕组:GA
文库,类型:5,更新时间:2013-10-2315:39:35,评论数:0,当前集数
:5,下载量:89,当前状态:连载中,本周下载量:4,标签,短评,轻小
说 ,下载列表,每行多列每行单列,1-放学后与白色魔女一起-,1.6MB,2-牙を剥く暗の睿智-,2.8MB,3-如神者-,1.4MB,4-蠢动的双头
蛇-,2.7MB,5-相克的摩天楼-,1.8MB,内容介绍,简介,卷入让街头
巷尾引起骚乱的连续杀人事件中并遭到杀害的藤间,大和，,被转
学生的冬云静马以魔术救回生命之后在她家取回了意识。,“喂，
为什么你是，全裸！？至少给我遮掩一下啊！？”,“为什么本小姐
不得不顾及你的想法啊。你才是，,既然在意的话就请把眼睛戳瞎
吧。”,“不对劲吧，那个！？”,大和虽然向自称为了阻止怪物引起
的连续杀人而来到这座小镇,的静马提出协助的请求，但是得到的
回答只有冰冷的拒绝。,即使如此大和依然追逐着静马闯入了超常
之夜，可是——,"
}
http://12club.nankai.edu.cn/programs/1227#intro
Traceback (most recent call last):
```

...乌鱼子。

而且更无语的是不知道为什么从昨天开始我的detailUrls.txt就莫名其妙清空了……

不过好在它从昨天开始就没有更新了，还是上次爬的时候的样子，倒也不用重新爬，从上次终止的地方做就行了。

![image-20211207081806509](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211207081806509.png)

还蛮好的，已经做了1996个了，好耶，应该很快就可以做完了，先再爬一次detailUrls.txt吧！我一定要做一个副本

![image-20211207093859704](C:/Users/16834/Desktop/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E5%AE%9E%E9%AA%8C/4%20%E5%AE%9E%E9%AA%8C4%2012.12/image-20211207093859704.png)

好的爬完了。

# NKmovie



# 南开大学官网

# 南开大学各学院官网

# 南开大学各公众号

